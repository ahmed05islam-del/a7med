{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmed05islam-del/a7med/blob/main/Pre_Preprocessing_For_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9VtiFtIzUk-",
        "outputId": "5fadc0cb-4aa2-41b6-88e5-12f5058e0311"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "help(nltk.download)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuyQNtxgzQ5U",
        "outputId": "f49cecba-b419-421e-d50e-4a1265773a02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method download in module nltk.downloader:\n",
            "\n",
            "download(info_or_id=None, download_dir=None, quiet=False, force=False, prefix='[nltk_data] ', halt_on_error=True, raise_on_error=False, print_error_to=<ipykernel.iostream.OutStream object at 0x785e6735ba00>) method of nltk.downloader.Downloader instance\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import downloader\n",
        "d = downloader.Downloader()\n",
        "for pkg in d.packages():\n",
        "    print(pkg)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hQ9viRtWzmwl",
        "outputId": "2e42f06c-9e20-4759-a08c-74e6d5d26718"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Package abc>\n",
            "<Package alpino>\n",
            "<Package averaged_perceptron_tagger>\n",
            "<Package averaged_perceptron_tagger_eng>\n",
            "<Package averaged_perceptron_tagger_ru>\n",
            "<Package averaged_perceptron_tagger_rus>\n",
            "<Package basque_grammars>\n",
            "<Package bcp47>\n",
            "<Package biocreative_ppi>\n",
            "<Package bllip_wsj_no_aux>\n",
            "<Package book_grammars>\n",
            "<Package brown>\n",
            "<Package brown_tei>\n",
            "<Package cess_cat>\n",
            "<Package cess_esp>\n",
            "<Package chat80>\n",
            "<Package city_database>\n",
            "<Package cmudict>\n",
            "<Package comparative_sentences>\n",
            "<Package comtrans>\n",
            "<Package conll2000>\n",
            "<Package conll2002>\n",
            "<Package conll2007>\n",
            "<Package crubadan>\n",
            "<Package dependency_treebank>\n",
            "<Package dolch>\n",
            "<Package english_wordnet>\n",
            "<Package europarl_raw>\n",
            "<Package extended_omw>\n",
            "<Package floresta>\n",
            "<Package framenet_v15>\n",
            "<Package framenet_v17>\n",
            "<Package gazetteers>\n",
            "<Package genesis>\n",
            "<Package gutenberg>\n",
            "<Package ieer>\n",
            "<Package inaugural>\n",
            "<Package indian>\n",
            "<Package jeita>\n",
            "<Package kimmo>\n",
            "<Package knbc>\n",
            "<Package large_grammars>\n",
            "<Package lin_thesaurus>\n",
            "<Package mac_morpho>\n",
            "<Package machado>\n",
            "<Package masc_tagged>\n",
            "<Package maxent_ne_chunker>\n",
            "<Package maxent_ne_chunker_tab>\n",
            "<Package maxent_treebank_pos_tagger>\n",
            "<Package maxent_treebank_pos_tagger_tab>\n",
            "<Package mock_corpus>\n",
            "<Package moses_sample>\n",
            "<Package movie_reviews>\n",
            "<Package mte_teip5>\n",
            "<Package mwa_ppdb>\n",
            "<Package names>\n",
            "<Package nombank.1.0>\n",
            "<Package nonbreaking_prefixes>\n",
            "<Package nps_chat>\n",
            "<Package omw>\n",
            "<Package omw-1.4>\n",
            "<Package opinion_lexicon>\n",
            "<Package panlex_swadesh>\n",
            "<Package paradigms>\n",
            "<Package pe08>\n",
            "<Package perluniprops>\n",
            "<Package pil>\n",
            "<Package pl196x>\n",
            "<Package porter_test>\n",
            "<Package ppattach>\n",
            "<Package problem_reports>\n",
            "<Package product_reviews_1>\n",
            "<Package product_reviews_2>\n",
            "<Package propbank>\n",
            "<Package pros_cons>\n",
            "<Package ptb>\n",
            "<Package punkt>\n",
            "<Package punkt_tab>\n",
            "<Package qc>\n",
            "<Package reuters>\n",
            "<Package rslp>\n",
            "<Package rte>\n",
            "<Package sample_grammars>\n",
            "<Package semcor>\n",
            "<Package senseval>\n",
            "<Package sentence_polarity>\n",
            "<Package sentiwordnet>\n",
            "<Package shakespeare>\n",
            "<Package sinica_treebank>\n",
            "<Package smultron>\n",
            "<Package snowball_data>\n",
            "<Package spanish_grammars>\n",
            "<Package state_union>\n",
            "<Package stopwords>\n",
            "<Package subjectivity>\n",
            "<Package swadesh>\n",
            "<Package switchboard>\n",
            "<Package tagsets>\n",
            "<Package tagsets_json>\n",
            "<Package timit>\n",
            "<Package toolbox>\n",
            "<Package treebank>\n",
            "<Package twitter_samples>\n",
            "<Package udhr>\n",
            "<Package udhr2>\n",
            "<Package unicode_samples>\n",
            "<Package universal_tagset>\n",
            "<Package universal_treebanks_v20>\n",
            "<Package vader_lexicon>\n",
            "<Package verbnet>\n",
            "<Package verbnet3>\n",
            "<Package webtext>\n",
            "<Package wmt15_eval>\n",
            "<Package word2vec_sample>\n",
            "<Package wordnet>\n",
            "<Package wordnet2021>\n",
            "<Package wordnet2022>\n",
            "<Package wordnet31>\n",
            "<Package wordnet_ic>\n",
            "<Package words>\n",
            "<Package ycoe>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTNs3AnW4xcS",
        "outputId": "3f9856e9-0435-4144-b22c-89f0a0a06088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "# Download the 'punkt' resource (tokenizer)\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EFciy_ce4xcW"
      },
      "outputs": [],
      "source": [
        "corpus=\"\"\"Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
        "Please do watch the entire course. to become expert in NLP.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNy96rdJ4xcX",
        "outputId": "f5a3892a-18d1-4ee3-c63b-9ff5735c436c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
            "Please do watch the entire course. to become expert in NLP.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vdafQpMt4xcX"
      },
      "outputs": [],
      "source": [
        "##  Tokenization\n",
        "## Sentence-->paragraphs\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wv-Lja1C4xcZ",
        "outputId": "bbd0ea50-a7e6-4fa2-ef2a-3052453a4859",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Now you can run your original code\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ6mVrr04xcZ",
        "outputId": "8cfef7ea-ec6a-4f6b-a4be-d0c7ef44d3d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Welcome,to Mohamed Atef NLP Tutorials.',\n",
              " 'Please do watch the entire course.',\n",
              " 'to become expert in NLP.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sITxKp5-4xcZ",
        "outputId": "e3fca3f4-7b63-4e1f-de8a-79727732d28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
            "1 Please do watch the entire course.\n",
            "2 to become expert in NLP.\n"
          ]
        }
      ],
      "source": [
        "num=0\n",
        "for sentence in documents:\n",
        "    print(num,sentence)\n",
        "    num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YCaHjO2g4xca"
      },
      "outputs": [],
      "source": [
        "## Tokenization\n",
        "## Paragraph-->words\n",
        "## sentence--->words\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drltLVzk4xca",
        "outputId": "017da218-a647-4918-e0f8-1df6a6fa2aa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Mohamed',\n",
              " 'Atef',\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '.',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maAQXDOc4xcb",
        "outputId": "dbb0169f-7856-4d2c-9cdd-bd249f68f438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'Mohamed', 'Atef', 'NLP', 'Tutorials', '.']\n",
            "['Please', 'do', 'watch', 'the', 'entire', 'course', '.']\n",
            "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "    print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMRpIcOyG-cX"
      },
      "source": [
        "# Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gdg0WuaGt60",
        "outputId": "d39ad012-fc01-458e-acd7-fa5c4d5dcc04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: [Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
            ", Please do watch the entire course., to become expert in NLP.\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define your text\n",
        "text = \"\"\"Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
        "Please do watch the entire course. to become expert in NLP.\n",
        "\"\"\"\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "# Tokenize into sentences\n",
        "sentences = [sent for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLmT0X3FH77I"
      },
      "source": [
        "## Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMKDVVtKHB4S",
        "outputId": "b9e45603-033b-496d-b3d9-3c33d09db40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: [Hello, Welcome, ,, to, Mohamed, Atef, NLP, Tutorials, ., \n",
            ", Please, do, watch, the, entire, course, ., to, become, expert, in, NLP, ., \n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define your text\n",
        "text =\"\"\"Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
        "Please do watch the entire course. to become expert in NLP.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize into words\n",
        "words = [token for token in doc]\n",
        "print(\"Word Tokens:\", words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WgNlh1d4mpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqklW9FHSFdK"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoexrY39SE2a",
        "outputId": "2242941d-831d-46cc-8604-93f11d8aff01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['cared', 'university', 'fairly', 'easily', 'singing', 'sings', 'sung', 'singer', 'sportingly', 'congratulations']\n",
            "Porter Stemmer: ['care', 'univers', 'fairli', 'easili', 'sing', 'sing', 'sung', 'singer', 'sportingli', 'congratul']\n",
            "Snowball Stemmer: ['care', 'univers', 'fair', 'easili', 'sing', 'sing', 'sung', 'singer', 'sport', 'congratul']\n",
            "Lancaster Stemmer: ['car', 'univers', 'fair', 'easy', 'sing', 'sing', 'sung', 'sing', 'sport', 'congrat']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "### Snowball Stemmer\n",
        "# It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "words = ['cared','university','fairly','easily','singing',\n",
        "       'sings','sung','singer','sportingly','congratulations']\n",
        "\n",
        "porter_stems = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "snowball_stems = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "lancaster_stems = [lancaster_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Porter Stemmer:\", porter_stems)\n",
        "print(\"Snowball Stemmer:\", snowball_stems)\n",
        "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
        "# congratulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem(\"better\"))\n",
        "print(stemmer.stem(\"was\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB6DaF2m5rAW",
        "outputId": "85ea7473-79ac-4648-b0f5-8e59651a894a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo_eaxxNa_CA"
      },
      "source": [
        "## Wordnet Lemmatizer\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWnRBd2OVJZY",
        "outputId": "cbfa402d-f292-4bbb-ecc5-060ef8dc0d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats: cat\n",
            "running: running\n",
            "better: better\n",
            "flies: fly\n",
            "congratulations: congratulation\n"
          ]
        }
      ],
      "source": [
        "## Q&A,chatbots,text summarization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['cats', 'running', 'better', 'flies','congratulations']\n",
        "for word in words:\n",
        "    print(f\"{word}: {lemmatizer.lemmatize(word)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet');\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize(\"better\", pos=\"a\"))   # → good\n",
        "print(lemma.lemmatize(\"was\", pos=\"v\"))      # → be"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UDVulg-4ysw",
        "outputId": "974bed7b-53c1-47cd-a307-2c68f8f093ec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "be\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\"n\" → noun\n",
        "# \"v\" → verb\n",
        "# \"a\" → adjective\n",
        "# \"r\" → adverb\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(\"better as Nono: \",lemma.lemmatize(\"better\"))             # بدون pos → better (اعتبرها noun)\n",
        "print(\"better as adjective: \",lemma.lemmatize(\"better\", pos=\"a\"))    # كصفة adjective → good\n",
        "\n",
        "print(\"was as Nono: \",lemma.lemmatize(\"was\"))                # بدون pos → was (ما قدر يرجع للجذر)\n",
        "print(\"was as verb: \",lemma.lemmatize(\"was\", pos=\"v\"))       # كفعل verb → be\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr78F3ip4-7n",
        "outputId": "f9c1f3f1-c95f-4640-c01b-fde7cafe5ec7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better as Nono:  better\n",
            "better as adjective:  good\n",
            "was as Nono:  wa\n",
            "was as verb:  be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEkmlDVxeAXB"
      },
      "source": [
        "# Stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2dD1khZSKG8",
        "outputId": "436743d0-07b8-497c-d17a-be91fc2ae7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words len :  399\n",
            "lemmatized_words len :  158\n",
            "three vision india 3000 year history people world come invaded u captured land conquered mind alexander onwards greek turk mogul portuguese british french dutch came looted u took yet done nation conquered anyone grabbed land culture history tried enforce way life respect freedom first vision freedom believe india got first vision 1857 started war independence freedom must protect nurture build free one respect u second vision india development fifty year developing nation time see developed nation among top 5 nation world term gdp 10 percent growth rate area poverty level falling achievement globally recognised today yet lack see developed nation incorrect third vision india must stand world believe unless india stand world one respect u strength respect strength must strong military power also economic power must go good fortune worked three great mind vikram sarabhai dept space professor satish dhawan succeeded brahm prakash father nuclear material lucky worked three closely consider great opportunity life see four milestone career\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\"\n",
        "\n",
        "paragraph = paragraph.lower()\n",
        "\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"words len : \",len(words))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"lemmatized_words len : \",len(lemmatized_words))\n",
        "\n",
        "processed_paragraph = ' '.join(lemmatized_words)\n",
        "\n",
        "print(processed_paragraph)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "aN1rFwkxoZD-",
        "outputId": "66a8666a-b7a8-4c3a-ff62-01b5bf9e1689"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'three vision india 3000 year history people world come invaded u captured land conquered mind alexander onwards greek turk mogul portuguese british french dutch came looted u took yet done nation conquered anyone grabbed land culture history tried enforce way life respect freedom first vision freedom believe india got first vision 1857 started war independence freedom must protect nurture build free one respect u second vision india development fifty year developing nation time see developed nation among top 5 nation world term gdp 10 percent growth rate area poverty level falling achievement globally recognised today yet lack see developed nation incorrect third vision india must stand world believe unless india stand world one respect u strength respect strength must strong military power also economic power must go good fortune worked three great mind vikram sarabhai dept space professor satish dhawan succeeded brahm prakash father nuclear material lucky worked three closely consider great opportunity life see four milestone career'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN4nNIWHnbf2",
        "outputId": "c96b4063-fc0a-429e-aa1c-907ce8b89789"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words(\"arabic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqdyrqOBnyO5",
        "outputId": "1efb6750-3ee2-40b5-ae99-583a5225188f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['إذ',\n",
              " 'إذا',\n",
              " 'إذما',\n",
              " 'إذن',\n",
              " 'أف',\n",
              " 'أقل',\n",
              " 'أكثر',\n",
              " 'ألا',\n",
              " 'إلا',\n",
              " 'التي',\n",
              " 'الذي',\n",
              " 'الذين',\n",
              " 'اللاتي',\n",
              " 'اللائي',\n",
              " 'اللتان',\n",
              " 'اللتيا',\n",
              " 'اللتين',\n",
              " 'اللذان',\n",
              " 'اللذين',\n",
              " 'اللواتي',\n",
              " 'إلى',\n",
              " 'إليك',\n",
              " 'إليكم',\n",
              " 'إليكما',\n",
              " 'إليكن',\n",
              " 'أم',\n",
              " 'أما',\n",
              " 'أما',\n",
              " 'إما',\n",
              " 'أن',\n",
              " 'إن',\n",
              " 'إنا',\n",
              " 'أنا',\n",
              " 'أنت',\n",
              " 'أنتم',\n",
              " 'أنتما',\n",
              " 'أنتن',\n",
              " 'إنما',\n",
              " 'إنه',\n",
              " 'أنى',\n",
              " 'أنى',\n",
              " 'آه',\n",
              " 'آها',\n",
              " 'أو',\n",
              " 'أولاء',\n",
              " 'أولئك',\n",
              " 'أوه',\n",
              " 'آي',\n",
              " 'أي',\n",
              " 'أيها',\n",
              " 'إي',\n",
              " 'أين',\n",
              " 'أين',\n",
              " 'أينما',\n",
              " 'إيه',\n",
              " 'بخ',\n",
              " 'بس',\n",
              " 'بعد',\n",
              " 'بعض',\n",
              " 'بك',\n",
              " 'بكم',\n",
              " 'بكم',\n",
              " 'بكما',\n",
              " 'بكن',\n",
              " 'بل',\n",
              " 'بلى',\n",
              " 'بما',\n",
              " 'بماذا',\n",
              " 'بمن',\n",
              " 'بنا',\n",
              " 'به',\n",
              " 'بها',\n",
              " 'بهم',\n",
              " 'بهما',\n",
              " 'بهن',\n",
              " 'بي',\n",
              " 'بين',\n",
              " 'بيد',\n",
              " 'تلك',\n",
              " 'تلكم',\n",
              " 'تلكما',\n",
              " 'ته',\n",
              " 'تي',\n",
              " 'تين',\n",
              " 'تينك',\n",
              " 'ثم',\n",
              " 'ثمة',\n",
              " 'حاشا',\n",
              " 'حبذا',\n",
              " 'حتى',\n",
              " 'حيث',\n",
              " 'حيثما',\n",
              " 'حين',\n",
              " 'خلا',\n",
              " 'دون',\n",
              " 'ذا',\n",
              " 'ذات',\n",
              " 'ذاك',\n",
              " 'ذان',\n",
              " 'ذانك',\n",
              " 'ذلك',\n",
              " 'ذلكم',\n",
              " 'ذلكما',\n",
              " 'ذلكن',\n",
              " 'ذه',\n",
              " 'ذو',\n",
              " 'ذوا',\n",
              " 'ذواتا',\n",
              " 'ذواتي',\n",
              " 'ذي',\n",
              " 'ذين',\n",
              " 'ذينك',\n",
              " 'ريث',\n",
              " 'سوف',\n",
              " 'سوى',\n",
              " 'شتان',\n",
              " 'عدا',\n",
              " 'عسى',\n",
              " 'عل',\n",
              " 'على',\n",
              " 'عليك',\n",
              " 'عليه',\n",
              " 'عما',\n",
              " 'عن',\n",
              " 'عند',\n",
              " 'غير',\n",
              " 'فإذا',\n",
              " 'فإن',\n",
              " 'فلا',\n",
              " 'فمن',\n",
              " 'في',\n",
              " 'فيم',\n",
              " 'فيما',\n",
              " 'فيه',\n",
              " 'فيها',\n",
              " 'قد',\n",
              " 'كأن',\n",
              " 'كأنما',\n",
              " 'كأي',\n",
              " 'كأين',\n",
              " 'كذا',\n",
              " 'كذلك',\n",
              " 'كل',\n",
              " 'كلا',\n",
              " 'كلاهما',\n",
              " 'كلتا',\n",
              " 'كلما',\n",
              " 'كليكما',\n",
              " 'كليهما',\n",
              " 'كم',\n",
              " 'كم',\n",
              " 'كما',\n",
              " 'كي',\n",
              " 'كيت',\n",
              " 'كيف',\n",
              " 'كيفما',\n",
              " 'لا',\n",
              " 'لاسيما',\n",
              " 'لدى',\n",
              " 'لست',\n",
              " 'لستم',\n",
              " 'لستما',\n",
              " 'لستن',\n",
              " 'لسن',\n",
              " 'لسنا',\n",
              " 'لعل',\n",
              " 'لك',\n",
              " 'لكم',\n",
              " 'لكما',\n",
              " 'لكن',\n",
              " 'لكنما',\n",
              " 'لكي',\n",
              " 'لكيلا',\n",
              " 'لم',\n",
              " 'لما',\n",
              " 'لن',\n",
              " 'لنا',\n",
              " 'له',\n",
              " 'لها',\n",
              " 'لهم',\n",
              " 'لهما',\n",
              " 'لهن',\n",
              " 'لو',\n",
              " 'لولا',\n",
              " 'لوما',\n",
              " 'لي',\n",
              " 'لئن',\n",
              " 'ليت',\n",
              " 'ليس',\n",
              " 'ليسا',\n",
              " 'ليست',\n",
              " 'ليستا',\n",
              " 'ليسوا',\n",
              " 'ما',\n",
              " 'ماذا',\n",
              " 'متى',\n",
              " 'مذ',\n",
              " 'مع',\n",
              " 'مما',\n",
              " 'ممن',\n",
              " 'من',\n",
              " 'منه',\n",
              " 'منها',\n",
              " 'منذ',\n",
              " 'مه',\n",
              " 'مهما',\n",
              " 'نحن',\n",
              " 'نحو',\n",
              " 'نعم',\n",
              " 'ها',\n",
              " 'هاتان',\n",
              " 'هاته',\n",
              " 'هاتي',\n",
              " 'هاتين',\n",
              " 'هاك',\n",
              " 'هاهنا',\n",
              " 'هذا',\n",
              " 'هذان',\n",
              " 'هذه',\n",
              " 'هذي',\n",
              " 'هذين',\n",
              " 'هكذا',\n",
              " 'هل',\n",
              " 'هلا',\n",
              " 'هم',\n",
              " 'هما',\n",
              " 'هن',\n",
              " 'هنا',\n",
              " 'هناك',\n",
              " 'هنالك',\n",
              " 'هو',\n",
              " 'هؤلاء',\n",
              " 'هي',\n",
              " 'هيا',\n",
              " 'هيت',\n",
              " 'هيهات',\n",
              " 'والذي',\n",
              " 'والذين',\n",
              " 'وإذ',\n",
              " 'وإذا',\n",
              " 'وإن',\n",
              " 'ولا',\n",
              " 'ولكن',\n",
              " 'ولو',\n",
              " 'وما',\n",
              " 'ومن',\n",
              " 'وهو',\n",
              " 'يا',\n",
              " 'أبٌ',\n",
              " 'أخٌ',\n",
              " 'حمٌ',\n",
              " 'فو',\n",
              " 'أنتِ',\n",
              " 'يناير',\n",
              " 'فبراير',\n",
              " 'مارس',\n",
              " 'أبريل',\n",
              " 'مايو',\n",
              " 'يونيو',\n",
              " 'يوليو',\n",
              " 'أغسطس',\n",
              " 'سبتمبر',\n",
              " 'أكتوبر',\n",
              " 'نوفمبر',\n",
              " 'ديسمبر',\n",
              " 'جانفي',\n",
              " 'فيفري',\n",
              " 'مارس',\n",
              " 'أفريل',\n",
              " 'ماي',\n",
              " 'جوان',\n",
              " 'جويلية',\n",
              " 'أوت',\n",
              " 'كانون',\n",
              " 'شباط',\n",
              " 'آذار',\n",
              " 'نيسان',\n",
              " 'أيار',\n",
              " 'حزيران',\n",
              " 'تموز',\n",
              " 'آب',\n",
              " 'أيلول',\n",
              " 'تشرين',\n",
              " 'دولار',\n",
              " 'دينار',\n",
              " 'ريال',\n",
              " 'درهم',\n",
              " 'ليرة',\n",
              " 'جنيه',\n",
              " 'قرش',\n",
              " 'مليم',\n",
              " 'فلس',\n",
              " 'هللة',\n",
              " 'سنتيم',\n",
              " 'يورو',\n",
              " 'ين',\n",
              " 'يوان',\n",
              " 'شيكل',\n",
              " 'واحد',\n",
              " 'اثنان',\n",
              " 'ثلاثة',\n",
              " 'أربعة',\n",
              " 'خمسة',\n",
              " 'ستة',\n",
              " 'سبعة',\n",
              " 'ثمانية',\n",
              " 'تسعة',\n",
              " 'عشرة',\n",
              " 'أحد',\n",
              " 'اثنا',\n",
              " 'اثني',\n",
              " 'إحدى',\n",
              " 'ثلاث',\n",
              " 'أربع',\n",
              " 'خمس',\n",
              " 'ست',\n",
              " 'سبع',\n",
              " 'ثماني',\n",
              " 'تسع',\n",
              " 'عشر',\n",
              " 'ثمان',\n",
              " 'سبت',\n",
              " 'أحد',\n",
              " 'اثنين',\n",
              " 'ثلاثاء',\n",
              " 'أربعاء',\n",
              " 'خميس',\n",
              " 'جمعة',\n",
              " 'أول',\n",
              " 'ثان',\n",
              " 'ثاني',\n",
              " 'ثالث',\n",
              " 'رابع',\n",
              " 'خامس',\n",
              " 'سادس',\n",
              " 'سابع',\n",
              " 'ثامن',\n",
              " 'تاسع',\n",
              " 'عاشر',\n",
              " 'حادي',\n",
              " 'أ',\n",
              " 'ب',\n",
              " 'ت',\n",
              " 'ث',\n",
              " 'ج',\n",
              " 'ح',\n",
              " 'خ',\n",
              " 'د',\n",
              " 'ذ',\n",
              " 'ر',\n",
              " 'ز',\n",
              " 'س',\n",
              " 'ش',\n",
              " 'ص',\n",
              " 'ض',\n",
              " 'ط',\n",
              " 'ظ',\n",
              " 'ع',\n",
              " 'غ',\n",
              " 'ف',\n",
              " 'ق',\n",
              " 'ك',\n",
              " 'ل',\n",
              " 'م',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ي',\n",
              " 'ء',\n",
              " 'ى',\n",
              " 'آ',\n",
              " 'ؤ',\n",
              " 'ئ',\n",
              " 'أ',\n",
              " 'ة',\n",
              " 'ألف',\n",
              " 'باء',\n",
              " 'تاء',\n",
              " 'ثاء',\n",
              " 'جيم',\n",
              " 'حاء',\n",
              " 'خاء',\n",
              " 'دال',\n",
              " 'ذال',\n",
              " 'راء',\n",
              " 'زاي',\n",
              " 'سين',\n",
              " 'شين',\n",
              " 'صاد',\n",
              " 'ضاد',\n",
              " 'طاء',\n",
              " 'ظاء',\n",
              " 'عين',\n",
              " 'غين',\n",
              " 'فاء',\n",
              " 'قاف',\n",
              " 'كاف',\n",
              " 'لام',\n",
              " 'ميم',\n",
              " 'نون',\n",
              " 'هاء',\n",
              " 'واو',\n",
              " 'ياء',\n",
              " 'همزة',\n",
              " 'ي',\n",
              " 'نا',\n",
              " 'ك',\n",
              " 'كن',\n",
              " 'ه',\n",
              " 'إياه',\n",
              " 'إياها',\n",
              " 'إياهما',\n",
              " 'إياهم',\n",
              " 'إياهن',\n",
              " 'إياك',\n",
              " 'إياكما',\n",
              " 'إياكم',\n",
              " 'إياك',\n",
              " 'إياكن',\n",
              " 'إياي',\n",
              " 'إيانا',\n",
              " 'أولالك',\n",
              " 'تانِ',\n",
              " 'تانِك',\n",
              " 'تِه',\n",
              " 'تِي',\n",
              " 'تَيْنِ',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'ذانِ',\n",
              " 'ذِه',\n",
              " 'ذِي',\n",
              " 'ذَيْنِ',\n",
              " 'هَؤلاء',\n",
              " 'هَاتانِ',\n",
              " 'هَاتِه',\n",
              " 'هَاتِي',\n",
              " 'هَاتَيْنِ',\n",
              " 'هَذا',\n",
              " 'هَذانِ',\n",
              " 'هَذِه',\n",
              " 'هَذِي',\n",
              " 'هَذَيْنِ',\n",
              " 'الألى',\n",
              " 'الألاء',\n",
              " 'أل',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'ذيت',\n",
              " 'كأيّ',\n",
              " 'كأيّن',\n",
              " 'بضع',\n",
              " 'فلان',\n",
              " 'وا',\n",
              " 'آمينَ',\n",
              " 'آهِ',\n",
              " 'آهٍ',\n",
              " 'آهاً',\n",
              " 'أُفٍّ',\n",
              " 'أُفٍّ',\n",
              " 'أفٍّ',\n",
              " 'أمامك',\n",
              " 'أمامكَ',\n",
              " 'أوّهْ',\n",
              " 'إلَيْكَ',\n",
              " 'إلَيْكَ',\n",
              " 'إليكَ',\n",
              " 'إليكنّ',\n",
              " 'إيهٍ',\n",
              " 'بخٍ',\n",
              " 'بسّ',\n",
              " 'بَسْ',\n",
              " 'بطآن',\n",
              " 'بَلْهَ',\n",
              " 'حاي',\n",
              " 'حَذارِ',\n",
              " 'حيَّ',\n",
              " 'حيَّ',\n",
              " 'دونك',\n",
              " 'رويدك',\n",
              " 'سرعان',\n",
              " 'شتانَ',\n",
              " 'شَتَّانَ',\n",
              " 'صهْ',\n",
              " 'صهٍ',\n",
              " 'طاق',\n",
              " 'طَق',\n",
              " 'عَدَسْ',\n",
              " 'كِخ',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانكم',\n",
              " 'مكانكما',\n",
              " 'مكانكنّ',\n",
              " 'نَخْ',\n",
              " 'هاكَ',\n",
              " 'هَجْ',\n",
              " 'هلم',\n",
              " 'هيّا',\n",
              " 'هَيْهات',\n",
              " 'وا',\n",
              " 'واهاً',\n",
              " 'وراءَك',\n",
              " 'وُشْكَانَ',\n",
              " 'وَيْ',\n",
              " 'يفعلان',\n",
              " 'تفعلان',\n",
              " 'يفعلون',\n",
              " 'تفعلون',\n",
              " 'تفعلين',\n",
              " 'اتخذ',\n",
              " 'ألفى',\n",
              " 'تخذ',\n",
              " 'ترك',\n",
              " 'تعلَّم',\n",
              " 'جعل',\n",
              " 'حجا',\n",
              " 'حبيب',\n",
              " 'خال',\n",
              " 'حسب',\n",
              " 'خال',\n",
              " 'درى',\n",
              " 'رأى',\n",
              " 'زعم',\n",
              " 'صبر',\n",
              " 'ظنَّ',\n",
              " 'عدَّ',\n",
              " 'علم',\n",
              " 'غادر',\n",
              " 'ذهب',\n",
              " 'وجد',\n",
              " 'ورد',\n",
              " 'وهب',\n",
              " 'أسكن',\n",
              " 'أطعم',\n",
              " 'أعطى',\n",
              " 'رزق',\n",
              " 'زود',\n",
              " 'سقى',\n",
              " 'كسا',\n",
              " 'أخبر',\n",
              " 'أرى',\n",
              " 'أعلم',\n",
              " 'أنبأ',\n",
              " 'حدَث',\n",
              " 'خبَّر',\n",
              " 'نبَّا',\n",
              " 'أفعل به',\n",
              " 'ما أفعله',\n",
              " 'بئس',\n",
              " 'ساء',\n",
              " 'طالما',\n",
              " 'قلما',\n",
              " 'لات',\n",
              " 'لكنَّ',\n",
              " 'ءَ',\n",
              " 'أجل',\n",
              " 'إذاً',\n",
              " 'أمّا',\n",
              " 'إمّا',\n",
              " 'إنَّ',\n",
              " 'أنًّ',\n",
              " 'أى',\n",
              " 'إى',\n",
              " 'أيا',\n",
              " 'ب',\n",
              " 'ثمَّ',\n",
              " 'جلل',\n",
              " 'جير',\n",
              " 'رُبَّ',\n",
              " 'س',\n",
              " 'علًّ',\n",
              " 'ف',\n",
              " 'كأنّ',\n",
              " 'كلَّا',\n",
              " 'كى',\n",
              " 'ل',\n",
              " 'لات',\n",
              " 'لعلَّ',\n",
              " 'لكنَّ',\n",
              " 'لكنَّ',\n",
              " 'م',\n",
              " 'نَّ',\n",
              " 'هلّا',\n",
              " 'وا',\n",
              " 'أل',\n",
              " 'إلّا',\n",
              " 'ت',\n",
              " 'ك',\n",
              " 'لمّا',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ا',\n",
              " 'ي',\n",
              " 'تجاه',\n",
              " 'تلقاء',\n",
              " 'جميع',\n",
              " 'حسب',\n",
              " 'سبحان',\n",
              " 'شبه',\n",
              " 'لعمر',\n",
              " 'مثل',\n",
              " 'معاذ',\n",
              " 'أبو',\n",
              " 'أخو',\n",
              " 'حمو',\n",
              " 'فو',\n",
              " 'مئة',\n",
              " 'مئتان',\n",
              " 'ثلاثمئة',\n",
              " 'أربعمئة',\n",
              " 'خمسمئة',\n",
              " 'ستمئة',\n",
              " 'سبعمئة',\n",
              " 'ثمنمئة',\n",
              " 'تسعمئة',\n",
              " 'مائة',\n",
              " 'ثلاثمائة',\n",
              " 'أربعمائة',\n",
              " 'خمسمائة',\n",
              " 'ستمائة',\n",
              " 'سبعمائة',\n",
              " 'ثمانمئة',\n",
              " 'تسعمائة',\n",
              " 'عشرون',\n",
              " 'ثلاثون',\n",
              " 'اربعون',\n",
              " 'خمسون',\n",
              " 'ستون',\n",
              " 'سبعون',\n",
              " 'ثمانون',\n",
              " 'تسعون',\n",
              " 'عشرين',\n",
              " 'ثلاثين',\n",
              " 'اربعين',\n",
              " 'خمسين',\n",
              " 'ستين',\n",
              " 'سبعين',\n",
              " 'ثمانين',\n",
              " 'تسعين',\n",
              " 'بضع',\n",
              " 'نيف',\n",
              " 'أجمع',\n",
              " 'جميع',\n",
              " 'عامة',\n",
              " 'عين',\n",
              " 'نفس',\n",
              " 'لا سيما',\n",
              " 'أصلا',\n",
              " 'أهلا',\n",
              " 'أيضا',\n",
              " 'بؤسا',\n",
              " 'بعدا',\n",
              " 'بغتة',\n",
              " 'تعسا',\n",
              " 'حقا',\n",
              " 'حمدا',\n",
              " 'خلافا',\n",
              " 'خاصة',\n",
              " 'دواليك',\n",
              " 'سحقا',\n",
              " 'سرا',\n",
              " 'سمعا',\n",
              " 'صبرا',\n",
              " 'صدقا',\n",
              " 'صراحة',\n",
              " 'طرا',\n",
              " 'عجبا',\n",
              " 'عيانا',\n",
              " 'غالبا',\n",
              " 'فرادى',\n",
              " 'فضلا',\n",
              " 'قاطبة',\n",
              " 'كثيرا',\n",
              " 'لبيك',\n",
              " 'معاذ',\n",
              " 'أبدا',\n",
              " 'إزاء',\n",
              " 'أصلا',\n",
              " 'الآن',\n",
              " 'أمد',\n",
              " 'أمس',\n",
              " 'آنفا',\n",
              " 'آناء',\n",
              " 'أنّى',\n",
              " 'أول',\n",
              " 'أيّان',\n",
              " 'تارة',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'حقا',\n",
              " 'صباح',\n",
              " 'مساء',\n",
              " 'ضحوة',\n",
              " 'عوض',\n",
              " 'غدا',\n",
              " 'غداة',\n",
              " 'قطّ',\n",
              " 'كلّما',\n",
              " 'لدن',\n",
              " 'لمّا',\n",
              " 'مرّة',\n",
              " 'قبل',\n",
              " 'خلف',\n",
              " 'أمام',\n",
              " 'فوق',\n",
              " 'تحت',\n",
              " 'يمين',\n",
              " 'شمال',\n",
              " 'ارتدّ',\n",
              " 'استحال',\n",
              " 'أصبح',\n",
              " 'أضحى',\n",
              " 'آض',\n",
              " 'أمسى',\n",
              " 'انقلب',\n",
              " 'بات',\n",
              " 'تبدّل',\n",
              " 'تحوّل',\n",
              " 'حار',\n",
              " 'رجع',\n",
              " 'راح',\n",
              " 'صار',\n",
              " 'ظلّ',\n",
              " 'عاد',\n",
              " 'غدا',\n",
              " 'كان',\n",
              " 'ما انفك',\n",
              " 'ما برح',\n",
              " 'مادام',\n",
              " 'مازال',\n",
              " 'مافتئ',\n",
              " 'ابتدأ',\n",
              " 'أخذ',\n",
              " 'اخلولق',\n",
              " 'أقبل',\n",
              " 'انبرى',\n",
              " 'أنشأ',\n",
              " 'أوشك',\n",
              " 'جعل',\n",
              " 'حرى',\n",
              " 'شرع',\n",
              " 'طفق',\n",
              " 'علق',\n",
              " 'قام',\n",
              " 'كرب',\n",
              " 'كاد',\n",
              " 'هبّ']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVsklYBQ6v-l",
        "outputId": "d9e4b645-270b-4268-8c40-c784a373917e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1050"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ7nAv777IpF",
        "outputId": "2e611bba-92f6-4d85-84e3-97a2cb5a694c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2228"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me3ArlcW8mE7"
      },
      "source": [
        "# POS(Part of speech)\n",
        "\n",
        "1- token.pos_ >>>>>>>represent pos(verp, noun,....)\n",
        "\n",
        "2- spacy.explain(token.pos_)   >>>>>akes the part-of-speech tag as an argument and returns a brief explanation of what the pos represents.\n",
        "\n",
        "3- token.tag_  >>>> tell me the tense(الزمن  بتاعى )\n",
        "\n",
        "4- spacy.explain(token.tag_ ) >>>> returns a brief explanation of what the tag represents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QHDRP5hjpJT",
        "outputId": "ba61944d-6a71-40ab-bbc4-b46ab10762f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon || PROPN || proper noun\n",
            "flew || VERB || verb\n",
            "to || ADP || adposition\n",
            "mars || NOUN || noun\n",
            "yesterday || NOUN || noun\n",
            ". || PUNCT || punctuation\n",
            "He || PRON || pronoun\n",
            "carried || VERB || verb\n",
            "biryani || ADJ || adjective\n",
            "masala || NOUN || noun\n",
            "with || ADP || adposition\n",
            "him || PRON || pronoun\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token,\"||\", token.pos_,\"||\", spacy.explain(token.pos_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n450mhf08pjn",
        "outputId": "f090dbf0-898e-408c-83eb-e5210db7cbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
            "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
            "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "Strange  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
            "265  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
            "the  |  DET  |  determiner  |  DT  |  determiner\n",
            "very  |  ADV  |  adverb  |  RB  |  adverb\n",
            "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
            "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "ApySsqsApAGR",
        "outputId": "0c77a843-1b4a-472c-e699-41e0bfb377a3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d7f3eea610a4407ba8a85599e71f4f6a-0\" class=\"displacy\" width=\"2150\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wow!</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Dr.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Strange</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">made</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">265</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">million</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">very</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">first</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">day</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-0\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-2\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-3\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-6\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1970.0,89.5 1970.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-7\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-8\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,177.0 1965.0,177.0 1965.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,266.5 L1812,254.5 1828,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1975.0,2.0 1975.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-d7f3eea610a4407ba8a85599e71f4f6a-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1975.0,266.5 L1983.0,254.5 1967.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeEzlVzi820l"
      },
      "source": [
        "## using pos as a filter for Removing all SPACE, PUNCT and X token from text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wK0bSY2T8xr-"
      },
      "outputs": [],
      "source": [
        "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
        "\n",
        "·         Revenue was $51.7 billion and increased 20%\n",
        "·         Operating income was $22.2 billion and increased 24%\n",
        "·         Net income was $18.8 billion and increased 21%\n",
        "·         Diluted earnings per share was $2.48 and increased 22%\n",
        "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
        "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
        "\n",
        "doc = nlp(earnings_text)\n",
        "\n",
        "filtered_tokens = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ not in [\"SPACE\", \"PUNCT\",\"NOUN\",\"VERB\"]:\n",
        "        filtered_tokens.append(token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    if token.pos_  in [\"NUM\"]:\n",
        "        print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzSFDnFjqJ1q",
        "outputId": "ee77360b-5b12-40b5-a4cf-d742963dc4c1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "2021\n",
            "51.7\n",
            "billion\n",
            "20\n",
            "22.2\n",
            "billion\n",
            "24\n",
            "18.8\n",
            "billion\n",
            "21\n",
            "2.48\n",
            "22\n",
            "22.1\n",
            "billion\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHWpAhL-pj-",
        "outputId": "11718c0a-6a8a-4395-8452-1eb71a39b6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108\n",
            "[Microsoft, Corp., the, for, the, December, 31, 2021, as, to, the, corresponding, of, last, fiscal, Revenue, was, $, 51.7, billion, and, 20, was, $, 22.2, billion, and, 24, Net, was, $, 18.8, billion, and, 21, per, was, $, 2.48, and, 22, Digital, is, the, most, malleable, at, the, ’s, to, and, everyday, and, Satya, Nadella, and, chief, executive, of, Microsoft, As, tech, as, a, of, global, GDP, to, we, are, and, across, diverse, and, with, a, common, and, an, that, a, common, and, of, Solid, commercial, by, strong, by, long, Azure, Microsoft, Cloud, to, $, 22.1, billion, up, 32, over, Amy, Hood, executive, and, chief, financial, of, Microsoft]\n"
          ]
        }
      ],
      "source": [
        "print(len(filtered_tokens))\n",
        "print(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABATZ6cY--Lh"
      },
      "source": [
        "## get the count of each pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI5c0XVW-524",
        "outputId": "43305cf2-9f5a-446b-a04a-8578a728a6b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{96: 15,\n",
              " 92: 45,\n",
              " 100: 23,\n",
              " 90: 9,\n",
              " 85: 16,\n",
              " 93: 16,\n",
              " 97: 27,\n",
              " 98: 1,\n",
              " 84: 20,\n",
              " 103: 10,\n",
              " 87: 6,\n",
              " 99: 5,\n",
              " 89: 12,\n",
              " 86: 3,\n",
              " 94: 3,\n",
              " 95: 2}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "count = doc.count_by(spacy.attrs.POS)\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k6ipTe5--_YM",
        "outputId": "95f029d8-2ff0-4cf5-bdda-16a48953b7a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PROPN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "doc.vocab[96].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHvWiqHN_C9i",
        "outputId": "a2601d39-f958-4515-e3ec-960c4742f63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROPN | 15\n",
            "NOUN | 45\n",
            "VERB | 23\n",
            "DET | 9\n",
            "ADP | 16\n",
            "NUM | 16\n",
            "PUNCT | 27\n",
            "SCONJ | 1\n",
            "ADJ | 20\n",
            "SPACE | 10\n",
            "AUX | 6\n",
            "SYM | 5\n",
            "CCONJ | 12\n",
            "ADV | 3\n",
            "PART | 3\n",
            "PRON | 2\n"
          ]
        }
      ],
      "source": [
        "for k,v in count.items():\n",
        "    print(doc.vocab[k].text, \"|\",v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wusYiljA_J4a"
      },
      "source": [
        "## Apply pos using ntlk not spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "nn2fOT3q_EGD",
        "outputId": "83d7dadd-5731-4e06-ad3b-50e37ddbcc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-232867278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Perform POS tagging on the tokenized words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Print the POS tags for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text into individual words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging on the tokenized words\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the POS tags for each word\n",
        "for word, tag in pos_tags:\n",
        "    print(word, tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwNIBDuUAFX0"
      },
      "source": [
        "### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3xKCWZV_M2r",
        "outputId": "9a051647-437f-48b7-9b1f-9eea629044dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
        "\"\"\"\n",
        "Person Eg: Krish C Naik\n",
        "Place Or Location Eg: India\n",
        "Date Eg: September,24-09-1989\n",
        "Time  Eg: 4:30pm\n",
        "Money Eg: 1 million dollar\n",
        "Organization Eg: iNeuron Private Limited\n",
        "Percent Eg: 20%, twenty percent\n",
        "\"\"\"\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1OqPv8xcAIcr"
      },
      "outputs": [],
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures. Meta is a social media and social networking service owned by American technology conglomerate Meta\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9NGGcqvfrCWS",
        "outputId": "d03cd4c2-3704-4e46-b41f-9c8a575ff461"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures. Meta is a social media and social networking service owned by American technology conglomerate Meta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "x2z0hcfgBp7T"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPQi-uRFBs2G",
        "outputId": "8aebbe60-de64-46b3-b42e-068ebfc4d5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Eiffel Tower  |  LOC  |  Non-GPE locations, mountain ranges, bodies of water\n",
            "1887 to 1889  |  DATE  |  Absolute or relative dates or periods\n",
            "Gustave Eiffel  |  PERSON  |  People, including fictional\n",
            "Meta  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "American  |  NORP  |  Nationalities or religious or political groups\n",
            "Meta  |  ORG  |  Companies, agencies, institutions, etc.\n"
          ]
        }
      ],
      "source": [
        "doc1 = nlp(sentence)\n",
        "for ent in doc1.ents:\n",
        "    print(ent ,\" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "t_GL5q3zBuj9",
        "outputId": "936bfd87-4b7a-4ab8-8b78-15eeda973576"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Eiffel Tower\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " was built from \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1887 to 1889\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gustave Eiffel\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", whose company specialized in building metal frameworks and structures. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Meta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is a social media and social networking service owned by \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " technology conglomerate \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Meta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc1, style=\"ent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full English Example"
      ],
      "metadata": {
        "id": "kGDI-qHn9oGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n"
      ],
      "metadata": {
        "id": "wpM5c_Nq7ae3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for resource in [\"punkt\", \"punkt_tab\", \"stopwords\", \"wordnet\", \"averaged_perceptron_tagger\",\"averaged_perceptron_tagger_eng\"]:\n",
        "    nltk.download(resource, quiet=True)\n"
      ],
      "metadata": {
        "id": "srMaxJBU7ah1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "the world have come and invaded us, captured our lands, conquered our minds.\n",
        "From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.\n",
        "Why? Because we respect the freedom of others.That is why my\n",
        "first vision is that of freedom. I believe that India got its first vision of\n",
        "this in 1857, when we started the War of Independence. It is this freedom that\n",
        "we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "I see four milestones in my career\"\"\"\n"
      ],
      "metadata": {
        "id": "DT1-zBfz7akn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = paragraph.lower()"
      ],
      "metadata": {
        "id": "0_CM_rJR7oN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "qr9ut5QI7oQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FMRv-ii7oT2",
        "outputId": "6e1906b1-5f90-4611-d1a6-ea9685a82113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'have', 'three', 'visions', 'for', 'india', '.', 'in', '3000', 'years', 'of', 'our', 'history', ',', 'people', 'from', 'all', 'over', 'the', 'world', 'have', 'come', 'and', 'invaded', 'us', ',', 'captured', 'our', 'lands', ',', 'conquered', 'our', 'minds', '.', 'from', 'alexander', 'onwards', ',', 'the', 'greeks', ',', 'the', 'turks', ',', 'the', 'moguls', ',', 'the', 'portuguese', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhLWRPNU7yKP",
        "outputId": "7250b4d7-5ac2-4f3e-ec4e-c5a1dd45419c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "399"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [word for word in words\n",
        "                  if re.match(\"^[a-zA-Z]+$\", word)\n",
        "                  and word not in stop_words]\n",
        "\n",
        "print(\"\\n📌 الكلمات بعد إزالة stopwords والرموز:\")\n",
        "print(filtered_words[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EQ7g0Vo7oWp",
        "outputId": "6dede0e0-71bd-4ec9-913b-dcb01f5a3a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 الكلمات بعد إزالة stopwords والرموز:\n",
            "['three', 'visions', 'india', 'years', 'history', 'people', 'world', 'come', 'invaded', 'us', 'captured', 'lands', 'conquered', 'minds', 'alexander', 'onwards', 'greeks', 'turks', 'moguls', 'portuguese', 'british', 'french', 'dutch', 'came', 'looted', 'us', 'took', 'yet', 'done', 'nation', 'conquered', 'anyone', 'grabbed', 'land', 'culture', 'history', 'tried', 'enforce', 'way', 'life', 'respect', 'freedom', 'first', 'vision', 'freedom', 'believe', 'india', 'got', 'first', 'vision']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wjf4INK71Ro",
        "outputId": "ac291375-44df-49b3-9faf-5adfbec11718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Lemmatization (مع POS Tagging)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "zKCn9RR17oZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "lLf8T3UQ771g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words = pos_tag(filtered_words)"
      ],
      "metadata": {
        "id": "PykTUL1v78GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words[:-50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1Eo-oSuJ78Ig",
        "outputId": "dd75b464-53b4-4007-ee0e-821939b29d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('three', 'CD'),\n",
              " ('visions', 'NNS'),\n",
              " ('india', 'VBP'),\n",
              " ('years', 'NNS'),\n",
              " ('history', 'NN'),\n",
              " ('people', 'NNS'),\n",
              " ('world', 'NN'),\n",
              " ('come', 'VBP'),\n",
              " ('invaded', 'VBN'),\n",
              " ('us', 'PRP'),\n",
              " ('captured', 'JJ'),\n",
              " ('lands', 'NNS'),\n",
              " ('conquered', 'VBD'),\n",
              " ('minds', 'NNS'),\n",
              " ('alexander', 'VBP'),\n",
              " ('onwards', 'NNS'),\n",
              " ('greeks', 'NNS'),\n",
              " ('turks', 'NNS'),\n",
              " ('moguls', 'VBP'),\n",
              " ('portuguese', 'JJ'),\n",
              " ('british', 'JJ'),\n",
              " ('french', 'JJ'),\n",
              " ('dutch', 'NN'),\n",
              " ('came', 'VBD'),\n",
              " ('looted', 'JJ'),\n",
              " ('us', 'PRP'),\n",
              " ('took', 'VBD'),\n",
              " ('yet', 'RB'),\n",
              " ('done', 'VBN'),\n",
              " ('nation', 'NN'),\n",
              " ('conquered', 'VBD'),\n",
              " ('anyone', 'NN'),\n",
              " ('grabbed', 'JJ'),\n",
              " ('land', 'JJ'),\n",
              " ('culture', 'NN'),\n",
              " ('history', 'NN'),\n",
              " ('tried', 'VBD'),\n",
              " ('enforce', 'JJ'),\n",
              " ('way', 'NN'),\n",
              " ('life', 'NN'),\n",
              " ('respect', 'NN'),\n",
              " ('freedom', 'NN'),\n",
              " ('first', 'JJ'),\n",
              " ('vision', 'NN'),\n",
              " ('freedom', 'NN'),\n",
              " ('believe', 'VBP'),\n",
              " ('india', 'NN'),\n",
              " ('got', 'VBD'),\n",
              " ('first', 'JJ'),\n",
              " ('vision', 'NN'),\n",
              " ('started', 'VBD'),\n",
              " ('war', 'NN'),\n",
              " ('independence', 'NN'),\n",
              " ('freedom', 'NN'),\n",
              " ('must', 'MD'),\n",
              " ('protect', 'VB'),\n",
              " ('nurture', 'NN'),\n",
              " ('build', 'VB'),\n",
              " ('free', 'JJ'),\n",
              " ('one', 'CD'),\n",
              " ('respect', 'NN'),\n",
              " ('us', 'PRP'),\n",
              " ('second', 'JJ'),\n",
              " ('vision', 'NN'),\n",
              " ('india', 'NN'),\n",
              " ('development', 'NN'),\n",
              " ('fifty', 'JJ'),\n",
              " ('years', 'NNS'),\n",
              " ('developing', 'VBG'),\n",
              " ('nation', 'NN'),\n",
              " ('time', 'NN'),\n",
              " ('see', 'VB'),\n",
              " ('developed', 'JJ'),\n",
              " ('nation', 'NN'),\n",
              " ('among', 'IN'),\n",
              " ('top', 'JJ'),\n",
              " ('nations', 'NNS'),\n",
              " ('world', 'NN'),\n",
              " ('terms', 'NNS'),\n",
              " ('gdp', 'VBP'),\n",
              " ('percent', 'JJ'),\n",
              " ('growth', 'NN'),\n",
              " ('rate', 'NN'),\n",
              " ('areas', 'NNS'),\n",
              " ('poverty', 'NN'),\n",
              " ('levels', 'NNS'),\n",
              " ('falling', 'VBG'),\n",
              " ('achievements', 'NNS'),\n",
              " ('globally', 'RB'),\n",
              " ('recognised', 'VBN'),\n",
              " ('today', 'NN'),\n",
              " ('yet', 'RB'),\n",
              " ('lack', 'JJ'),\n",
              " ('see', 'NN'),\n",
              " ('developed', 'JJ'),\n",
              " ('nation', 'NN'),\n",
              " ('incorrect', 'JJ'),\n",
              " ('third', 'JJ'),\n",
              " ('vision', 'NN'),\n",
              " ('india', 'NN'),\n",
              " ('must', 'MD'),\n",
              " ('stand', 'VB'),\n",
              " ('world', 'NN'),\n",
              " ('believe', 'VBP')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "                    for word, tag in tagged_words]"
      ],
      "metadata": {
        "id": "INchM5Bi78Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kdpmMTi778Nh",
        "outputId": "cc699f4f-2999-4644-edc2-d9b1181be024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three',\n",
              " 'vision',\n",
              " 'india',\n",
              " 'year',\n",
              " 'history',\n",
              " 'people',\n",
              " 'world',\n",
              " 'come',\n",
              " 'invade',\n",
              " 'u',\n",
              " 'captured',\n",
              " 'land',\n",
              " 'conquer',\n",
              " 'mind',\n",
              " 'alexander',\n",
              " 'onwards',\n",
              " 'greek',\n",
              " 'turk',\n",
              " 'moguls',\n",
              " 'portuguese',\n",
              " 'british',\n",
              " 'french',\n",
              " 'dutch',\n",
              " 'come',\n",
              " 'looted',\n",
              " 'u',\n",
              " 'take',\n",
              " 'yet',\n",
              " 'do',\n",
              " 'nation',\n",
              " 'conquer',\n",
              " 'anyone',\n",
              " 'grabbed',\n",
              " 'land',\n",
              " 'culture',\n",
              " 'history',\n",
              " 'try',\n",
              " 'enforce',\n",
              " 'way',\n",
              " 'life',\n",
              " 'respect',\n",
              " 'freedom',\n",
              " 'first',\n",
              " 'vision',\n",
              " 'freedom',\n",
              " 'believe',\n",
              " 'india',\n",
              " 'get',\n",
              " 'first',\n",
              " 'vision',\n",
              " 'start',\n",
              " 'war',\n",
              " 'independence',\n",
              " 'freedom',\n",
              " 'must',\n",
              " 'protect',\n",
              " 'nurture',\n",
              " 'build',\n",
              " 'free',\n",
              " 'one',\n",
              " 'respect',\n",
              " 'u',\n",
              " 'second',\n",
              " 'vision',\n",
              " 'india',\n",
              " 'development',\n",
              " 'fifty',\n",
              " 'year',\n",
              " 'develop',\n",
              " 'nation',\n",
              " 'time',\n",
              " 'see',\n",
              " 'developed',\n",
              " 'nation',\n",
              " 'among',\n",
              " 'top',\n",
              " 'nation',\n",
              " 'world',\n",
              " 'term',\n",
              " 'gdp',\n",
              " 'percent',\n",
              " 'growth',\n",
              " 'rate',\n",
              " 'area',\n",
              " 'poverty',\n",
              " 'level',\n",
              " 'fall',\n",
              " 'achievement',\n",
              " 'globally',\n",
              " 'recognise',\n",
              " 'today',\n",
              " 'yet',\n",
              " 'lack',\n",
              " 'see',\n",
              " 'developed',\n",
              " 'nation',\n",
              " 'incorrect',\n",
              " 'third',\n",
              " 'vision',\n",
              " 'india',\n",
              " 'must',\n",
              " 'stand',\n",
              " 'world',\n",
              " 'believe',\n",
              " 'unless',\n",
              " 'india',\n",
              " 'stand',\n",
              " 'world',\n",
              " 'one',\n",
              " 'respect',\n",
              " 'u',\n",
              " 'strength',\n",
              " 'respect',\n",
              " 'strength',\n",
              " 'must',\n",
              " 'strong',\n",
              " 'military',\n",
              " 'power',\n",
              " 'also',\n",
              " 'economic',\n",
              " 'power',\n",
              " 'must',\n",
              " 'go',\n",
              " 'good',\n",
              " 'fortune',\n",
              " 'work',\n",
              " 'three',\n",
              " 'great',\n",
              " 'mind',\n",
              " 'vikram',\n",
              " 'sarabhai',\n",
              " 'dept',\n",
              " 'space',\n",
              " 'professor',\n",
              " 'satish',\n",
              " 'dhawan',\n",
              " 'succeed',\n",
              " 'brahm',\n",
              " 'prakash',\n",
              " 'father',\n",
              " 'nuclear',\n",
              " 'material',\n",
              " 'lucky',\n",
              " 'work',\n",
              " 'three',\n",
              " 'closely',\n",
              " 'consider',\n",
              " 'great',\n",
              " 'opportunity',\n",
              " 'life',\n",
              " 'see',\n",
              " 'four',\n",
              " 'milestone',\n",
              " 'career']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cats are running better than the dogs\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [(word, tag, lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
        "              for word, tag in tagged]\n",
        "lemmatized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLuDrMFW78Qa",
        "outputId": "53ba9b2f-86f6-4f0f-f05e-779f8dde679d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT', 'The'),\n",
              " ('cats', 'NNS', 'cat'),\n",
              " ('are', 'VBP', 'be'),\n",
              " ('running', 'VBG', 'run'),\n",
              " ('better', 'JJR', 'good'),\n",
              " ('than', 'IN', 'than'),\n",
              " ('the', 'DT', 'the'),\n",
              " ('dogs', 'NNS', 'dog')]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stems = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "print(\"\\n comparison Stemming vs Lemmatization:\")\n",
        "for i in range(20):\n",
        "    print(f\"{filtered_words[i]:15} | Stem: {stems[i]:10} | Lemma: {lemmatized_words[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx5617Vx9YS5",
        "outputId": "2646fa69-8738-44ee-85c6-e7d4536c359b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " comparison Stemming vs Lemmatization:\n",
            "three           | Stem: three      | Lemma: three\n",
            "visions         | Stem: vision     | Lemma: vision\n",
            "india           | Stem: india      | Lemma: india\n",
            "years           | Stem: year       | Lemma: year\n",
            "history         | Stem: histori    | Lemma: history\n",
            "people          | Stem: peopl      | Lemma: people\n",
            "world           | Stem: world      | Lemma: world\n",
            "come            | Stem: come       | Lemma: come\n",
            "invaded         | Stem: invad      | Lemma: invade\n",
            "us              | Stem: us         | Lemma: u\n",
            "captured        | Stem: captur     | Lemma: captured\n",
            "lands           | Stem: land       | Lemma: land\n",
            "conquered       | Stem: conquer    | Lemma: conquer\n",
            "minds           | Stem: mind       | Lemma: mind\n",
            "alexander       | Stem: alexand    | Lemma: alexander\n",
            "onwards         | Stem: onward     | Lemma: onwards\n",
            "greeks          | Stem: greek      | Lemma: greek\n",
            "turks           | Stem: turk       | Lemma: turk\n",
            "moguls          | Stem: mogul      | Lemma: moguls\n",
            "portuguese      | Stem: portugues  | Lemma: portuguese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_paragraph = ' '.join(lemmatized_words)\n",
        "print(\"\\n Final Text\")\n",
        "print(processed_paragraph[:500], \"...\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"original\": filtered_words,\n",
        "    \"lemma\": lemmatized_words,\n",
        "    \"stem\": stems\n",
        "})\n",
        "df.to_csv(\"cleaned_tokens.csv\", index=False)\n",
        "print(\"\\n Saved in cleaned_tokens.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrNY_NuD9rVW",
        "outputId": "b791bddc-568b-4a1c-8e97-30d04a6538c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Text\n",
            "three vision india year history people world come invade u captured land conquer mind alexander onwards greek turk moguls portuguese british french dutch come looted u take yet do nation conquer anyone grabbed land culture history try enforce way life respect freedom first vision freedom believe india get first vision start war independence freedom must protect nurture build free one respect u second vision india development fifty year develop nation time see developed nation among top nation wo ...\n",
            "\n",
            " Saved in cleaned_tokens.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Arabic Example"
      ],
      "metadata": {
        "id": "3oFX6HTp_J-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 🟢 1) Import libraries\n",
        "# ================================\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources\n",
        "for resource in [\"punkt\", \"punkt_tab\"]:\n",
        "    nltk.download(resource, quiet=True)\n",
        "\n",
        "# ================================\n",
        "# 🟡 2) Define Arabic text\n",
        "# ================================\n",
        "paragraph = \"\"\"التعليم هو أساس تقدم أي مجتمع. عندما يستثمر الناس في العلم والمعرفة،\n",
        "فإنهم يفتحون أبواب المستقبل أمام الأجيال القادمة. في السنوات الأخيرة،\n",
        "شهدنا تطوراً كبيراً في مجال الذكاء الاصطناعي، وأصبح جزءاً أساسياً من حياتنا اليومية،\n",
        "ابتداءً من الهواتف الذكية وصولاً إلى تطبيقات الرعاية الصحية. ومع ذلك،\n",
        "يبقى التحدي الأكبر هو كيفية استخدام هذه التقنيات بشكل مسؤول يضمن\n",
        "العدالة والشفافية ويخدم الإنسانية بأفضل صورة.\n",
        "\"\"\"\n",
        "\n",
        "# ================================\n",
        "# 🔵 3) Tokenization (split into words)\n",
        "# ================================\n",
        "words = word_tokenize(paragraph)\n",
        "\n",
        "print(\"📌 Words before cleaning:\")\n",
        "print(words)\n",
        "print(len(words))\n",
        "\n",
        "# ================================\n",
        "# 🟣 4) Remove stopwords + symbols\n",
        "# ================================\n",
        "# Custom Arabic stopwords (can be extended)\n",
        "arabic_stopwords = set([\n",
        "    \"في\",\"من\",\"على\",\"إلى\",\"عن\",\"مع\",\"كان\",\"التي\",\"الذي\",\n",
        "    \"هذا\",\"هذه\",\"ذلك\",\"تلك\",\"هناك\",\"كل\",\"لم\",\"لن\",\"ما\",\"لا\",\n",
        "    \"إن\",\"أن\",\"قد\",\"حتى\",\"ثم\",\"به\",\"بها\",\"هو\",\"هي\",\"هم\",\"هن\",\n",
        "    \"أنا\",\"نحن\",\"انت\",\"أنت\",\"انتم\",\"انتن\"\n",
        "])\n",
        "\n",
        "# Keep only Arabic letters and remove stopwords\n",
        "filtered_words = [word for word in words\n",
        "                  if re.match(\"^[\\u0621-\\u064A]+$\", word)\n",
        "                  and word not in arabic_stopwords]\n",
        "\n",
        "print(\"\\n📌 Words after removing stopwords and symbols:\")\n",
        "print(filtered_words)\n",
        "print(len(filtered_words))\n",
        "\n",
        "# ================================\n",
        "# 🟤 5) (Optional) Stemming with ISRIStemmer\n",
        "# ================================\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "stems = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "print(\"\\n📌 Example: Stemming Arabic words\")\n",
        "for i in range(min(15, len(filtered_words))):\n",
        "    print(f\"{filtered_words[i]:15} | Stem: {stems[i]}\")\n",
        "\n",
        "# ================================\n",
        "# 🔴 6) Final cleaned text + Save\n",
        "# ================================\n",
        "processed_paragraph = \" \".join(stems)\n",
        "print(\"\\n📌 Final processed Arabic text:\")\n",
        "print(processed_paragraph)\n",
        "\n",
        "# Save results to CSV\n",
        "df = pd.DataFrame({\n",
        "    \"original\": filtered_words,\n",
        "    \"stem\": stems\n",
        "})\n",
        "df.to_csv(\"cleaned_tokens_ar.csv\", index=False)\n",
        "print(\"\\n✅ Results saved to cleaned_tokens_ar.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGvllq6696kw",
        "outputId": "f6bd6502-8ec8-4558-9c15-68dc287e05a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Words before cleaning:\n",
            "['التعليم', 'هو', 'أساس', 'تقدم', 'أي', 'مجتمع', '.', 'عندما', 'يستثمر', 'الناس', 'في', 'العلم', 'والمعرفة،', 'فإنهم', 'يفتحون', 'أبواب', 'المستقبل', 'أمام', 'الأجيال', 'القادمة', '.', 'في', 'السنوات', 'الأخيرة،', 'شهدنا', 'تطوراً', 'كبيراً', 'في', 'مجال', 'الذكاء', 'الاصطناعي،', 'وأصبح', 'جزءاً', 'أساسياً', 'من', 'حياتنا', 'اليومية،', 'ابتداءً', 'من', 'الهواتف', 'الذكية', 'وصولاً', 'إلى', 'تطبيقات', 'الرعاية', 'الصحية', '.', 'ومع', 'ذلك،', 'يبقى', 'التحدي', 'الأكبر', 'هو', 'كيفية', 'استخدام', 'هذه', 'التقنيات', 'بشكل', 'مسؤول', 'يضمن', 'العدالة', 'والشفافية', 'ويخدم', 'الإنسانية', 'بأفضل', 'صورة', '.']\n",
            "67\n",
            "\n",
            "📌 Words after removing stopwords and symbols:\n",
            "['التعليم', 'أساس', 'تقدم', 'أي', 'مجتمع', 'عندما', 'يستثمر', 'الناس', 'العلم', 'فإنهم', 'يفتحون', 'أبواب', 'المستقبل', 'أمام', 'الأجيال', 'القادمة', 'السنوات', 'شهدنا', 'مجال', 'الذكاء', 'وأصبح', 'حياتنا', 'الهواتف', 'الذكية', 'تطبيقات', 'الرعاية', 'الصحية', 'ومع', 'يبقى', 'التحدي', 'الأكبر', 'كيفية', 'استخدام', 'التقنيات', 'بشكل', 'مسؤول', 'يضمن', 'العدالة', 'والشفافية', 'ويخدم', 'الإنسانية', 'بأفضل', 'صورة']\n",
            "43\n",
            "\n",
            "📌 Example: Stemming Arabic words\n",
            "التعليم         | Stem: علم\n",
            "أساس            | Stem: اسس\n",
            "تقدم            | Stem: قدم\n",
            "أي              | Stem: اي\n",
            "مجتمع           | Stem: جمع\n",
            "عندما           | Stem: عند\n",
            "يستثمر          | Stem: ثمر\n",
            "الناس           | Stem: ناس\n",
            "العلم           | Stem: علم\n",
            "فإنهم           | Stem: فإن\n",
            "يفتحون          | Stem: فتح\n",
            "أبواب           | Stem: بوب\n",
            "المستقبل        | Stem: قبل\n",
            "أمام            | Stem: أمام\n",
            "الأجيال         | Stem: جيل\n",
            "\n",
            "📌 Final processed Arabic text:\n",
            "علم اسس قدم اي جمع عند ثمر ناس علم فإن فتح بوب قبل أمام جيل قدم سنو شهد جال ذكء أصبح حيت هتف ذكة طبق رعي صحة ومع بقى تحد كبر كيف خدم تقن شكل سؤل يضم عدل شفف خدم سان أفضل صور\n",
            "\n",
            "✅ Results saved to cleaned_tokens_ar.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "viPqJuSysvF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Lab 1 NLP Text Preprocessing Assignment (NLTK + SpyCy)\n",
        "\n",
        "# 6:40 PM to 7:00 Break\n",
        "# 7:00 to 9:00 Practical  \n",
        "\n",
        "## Objective\n",
        "This assignment aims to assess your understanding of **text preprocessing** and **linguistic analysis** techniques using Python and the NLTK library.\n",
        "\n",
        "You will apply multiple NLP steps to raw text data in preparation for **machine learning tasks** such as **text classification** or **sentiment analysis**.\n",
        "---\n",
        "\n",
        "## 📄 Given Text Data\n",
        "Use the following dataset in your assignment:\n",
        "\n",
        "```python\n",
        "text_data = [\n",
        "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
        "    \"I hated the film, it was the worst I have ever seen\",\n",
        "    \"The storyline was boring but the acting was brilliant\",\n",
        "    \"An amazing movie with a great plot and incredible performances\",\n",
        "    \"Egypt movie, I regret wasting my time on it\",\n",
        "    \"The actors did a great job but the story lacked depth\",\n",
        "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
        "    \"This film was just okay, not too bad but not great either\",\n",
        "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
        "    \"The movie was disappointing, it did not live up to the hype\"\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "##  Tasks\n",
        "\n",
        "### 1️⃣ Tokenization\n",
        "**Task:**\n",
        "- Convert all sentences to lowercase.\n",
        "- Tokenize each sentence into individual words.\n",
        "\n",
        "📌 *Hint:* Use `nltk.tokenize.word_tokenize`\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Stopword Removal\n",
        "**Task:**\n",
        "- Remove English stopwords from the tokenized text.\n",
        "- Remove punctuation and non-alphabetic tokens.\n",
        "\n",
        "📌 *Hint:* Use `nltk.corpus.stopwords`\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ Stemming\n",
        "**Task:**\n",
        "- Apply stemming to the cleaned tokens.\n",
        "\n",
        "📌 *Hint:* Use `nltk.stem.PorterStemmer`\n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ Lemmatization\n",
        "**Task:**\n",
        "- Apply lemmatization to the cleaned tokens.\n",
        "\n",
        "📌 *Hint:* Use `nltk.stem.WordNetLemmatizer`\n",
        "\n",
        "---\n",
        "\n",
        "### 5️⃣ Part-of-Speech (POS) Tagging\n",
        "**Task:**\n",
        "- Perform POS tagging on the lemmatized tokens.\n",
        "\n",
        "📌 *Hint:* Use `nltk.pos_tag`\n",
        "\n",
        "---\n",
        "\n",
        "### 6️⃣ Named Entity Recognition (NER)\n",
        "**Task:**\n",
        "- Identify named entities in the text.\n",
        "- Focus on entities such as **locations**, **organizations**, or **names**.\n",
        "\n",
        "📌 *Hint:* Use `nltk.ne_chunk`\n",
        "\n",
        "---\n",
        "\n",
        "### 7️⃣ Save Results to CSV\n",
        "**Task:**\n",
        "- Create a pandas DataFrame containing:\n",
        "  - Original text\n",
        "  - Tokenized text\n",
        "  - Cleaned tokens\n",
        "  - Stemmed tokens\n",
        "  - Lemmatized tokens\n",
        "  - POS tags\n",
        "- Save the DataFrame to a CSV file named:\n",
        "\n",
        "```\n",
        "nlp_assignment_results.csv\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Happy Coding 💻\n",
        "\n"
      ],
      "metadata": {
        "id": "d3pSMgVjVio_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [\n",
        "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
        "    \"I hated the film, it was the worst I have ever seen\",\n",
        "    \"The storyline was boring but the acting was brilliant\",\n",
        "    \"An amazing movie with a great plot and incredible performances\",\n",
        "    \"Egypt movie, I regret wasting my time on it\",\n",
        "    \"The actors did a great job but the story lacked depth\",\n",
        "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
        "    \"This film was just okay, not too bad but not great either\",\n",
        "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
        "    \"The movie was disappointing, it did not live up to the hype\"\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hVTnl6vkst1L"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "tokenized_docs = [word_tokenize(sentence.lower()) for sentence in documents]\n",
        "print(tokenized_docs[0])"
      ],
      "metadata": {
        "id": "XMQ2HNl9yTfP",
        "outputId": "c63874ce-819f-4f1a-822d-239e5b0cce2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'welcome', ',', 'to', 'mohamed', 'atef', 'nlp', 'tutorials', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#2\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "e\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "filtered_docs = [\n",
        "    [word for word in sentence if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    for sentence in tokenized_docs\n",
        "]\n",
        "print(f\"Original: {tokenized_docs[0]}\")\n",
        "print(f\"Cleaned:  {filtered_docs[0]}\")"
      ],
      "metadata": {
        "id": "NzE8iiI8mBmZ",
        "outputId": "f7796f51-36a7-4232-e3d6-f642feda1733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['hello', 'welcome', ',', 'to', 'mohamed', 'atef', 'nlp', 'tutorials', '.']\n",
            "Cleaned:  ['hello', 'welcome', 'mohamed', 'atef', 'nlp', 'tutorials']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#3\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_docs = [\n",
        "    [stemmer.stem(word) for word in sentence]\n",
        "    for sentence in filtered_docs\n",
        "]\n",
        "print(f\"Cleaned Tokens: {filtered_docs[0]}\")\n",
        "print(f\"Stemmed Tokens: {stemmed_docs[0]}\")"
      ],
      "metadata": {
        "id": "kijsH5-9nhGu",
        "outputId": "daebcbcd-12a7-4690-c4d7-b4a80907e880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens: ['hello', 'welcome', 'mohamed', 'atef', 'nlp', 'tutorials']\n",
            "Stemmed Tokens: ['hello', 'welcom', 'moham', 'atef', 'nlp', 'tutori']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#4\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_docs = [\n",
        "    [lemmatizer.lemmatize(word) for word in sentence]\n",
        "    for sentence in filtered_docs\n",
        "]\n",
        "print(f\"Filtered (Cleaned): {filtered_docs[0]}\")\n",
        "print(f\"Lemmatized:        {lemmatized_docs[0]}\")"
      ],
      "metadata": {
        "id": "xsVah5VInmZs",
        "outputId": "55d1f295-2fbe-4dff-d07a-8875924297cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered (Cleaned): ['hello', 'welcome', 'mohamed', 'atef', 'nlp', 'tutorials']\n",
            "Lemmatized:        ['hello', 'welcome', 'mohamed', 'atef', 'nlp', 'tutorial']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tagged_docs = [\n",
        "    nltk.pos_tag(sentence)\n",
        "    for sentence in lemmatized_docs\n",
        "]\n",
        "print(f\"Lemmatized Words: {lemmatized_docs[0]}\")\n",
        "print(f\"POS Tags:         {pos_tagged_docs[0]}\")"
      ],
      "metadata": {
        "id": "ffGQRnWFocmw",
        "outputId": "1a862934-5270-4822-cc1c-9ef914d61793",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['hello', 'welcome', 'mohamed', 'atef', 'nlp', 'tutorial']\n",
            "POS Tags:         [('hello', 'JJ'), ('welcome', 'NN'), ('mohamed', 'VBD'), ('atef', 'JJ'), ('nlp', 'JJ'), ('tutorial', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "import nltk\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "sentence = text_data[0]\n",
        "tokens = word_tokenize(sentence)\n",
        "tags = pos_tag(tokens)\n",
        "entities = ne_chunk(tags)\n",
        "\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "H5yzd2ODoleT",
        "outputId": "4b5302fb-5f6c-4dc2-9434-92a39863675f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  movie/NN\n",
            "  was/VBD\n",
            "  fantastic/JJ\n",
            "  and/CC\n",
            "  I/PRP\n",
            "  loved/VBD\n",
            "  every/DT\n",
            "  part/NN\n",
            "  of/IN\n",
            "  it/PRP\n",
            "  about/IN\n",
            "  (GPE Egypt/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "results = []\n",
        "for text in text_data:\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned = [w.lower() for w in tokens if w.isalpha() and w.lower() not in stop_words]\n",
        "    stemmed = [stemmer.stem(w) for w in cleaned]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###\n",
        "\n",
        "\n",
        "    lemmatized = [lemmatizer.lemmatize(w) for w in cleaned]\n",
        "\n",
        "\n",
        "    pos_tags = nltk.pos_tag(lemmatized)\n",
        "\n",
        "\n",
        "    results.append({\n",
        "        \"Original Text\": text,\n",
        "        \"Tokenized\": tokens,\n",
        "        \"Cleaned\": cleaned,\n",
        "        \"Stemmed\": stemmed,\n",
        "        \"Lemmatized\": lemmatized,\n",
        "        \"POS Tags\": pos_tags\n",
        "    })\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"nlp_preprocessing_results.csv\", index=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zTArxVswo09v",
        "outputId": "f140133b-a0f1-4b3c-cbf3-2804d26c4833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       Original Text  \\\n",
              "0  The movie was fantastic and I loved every part...   \n",
              "1  I hated the film, it was the worst I have ever...   \n",
              "2  The storyline was boring but the acting was br...   \n",
              "3  An amazing movie with a great plot and incredi...   \n",
              "4        Egypt movie, I regret wasting my time on it   \n",
              "\n",
              "                                           Tokenized  \\\n",
              "0  [The, movie, was, fantastic, and, I, loved, ev...   \n",
              "1  [I, hated, the, film, ,, it, was, the, worst, ...   \n",
              "2  [The, storyline, was, boring, but, the, acting...   \n",
              "3  [An, amazing, movie, with, a, great, plot, and...   \n",
              "4  [Egypt, movie, ,, I, regret, wasting, my, time...   \n",
              "\n",
              "                                             Cleaned  \\\n",
              "0      [movie, fantastic, loved, every, part, egypt]   \n",
              "1                   [hated, film, worst, ever, seen]   \n",
              "2             [storyline, boring, acting, brilliant]   \n",
              "3  [amazing, movie, great, plot, incredible, perf...   \n",
              "4              [egypt, movie, regret, wasting, time]   \n",
              "\n",
              "                                      Stemmed  \\\n",
              "0   [movi, fantast, love, everi, part, egypt]   \n",
              "1             [hate, film, worst, ever, seen]   \n",
              "2            [storylin, bore, act, brilliant]   \n",
              "3  [amaz, movi, great, plot, incred, perform]   \n",
              "4           [egypt, movi, regret, wast, time]   \n",
              "\n",
              "                                          Lemmatized  \\\n",
              "0      [movie, fantastic, loved, every, part, egypt]   \n",
              "1                   [hated, film, worst, ever, seen]   \n",
              "2             [storyline, boring, acting, brilliant]   \n",
              "3  [amazing, movie, great, plot, incredible, perf...   \n",
              "4              [egypt, movie, regret, wasting, time]   \n",
              "\n",
              "                                            POS Tags  \n",
              "0  [(movie, NN), (fantastic, JJ), (loved, VBN), (...  \n",
              "1  [(hated, VBN), (film, NN), (worst, JJS), (ever...  \n",
              "2  [(storyline, NN), (boring, VBG), (acting, VBG)...  \n",
              "3  [(amazing, JJ), (movie, NN), (great, JJ), (plo...  \n",
              "4  [(egypt, JJ), (movie, NN), (regret, VB), (wast...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b472edc-68ad-410c-86b3-2e0a6162b8b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Text</th>\n",
              "      <th>Tokenized</th>\n",
              "      <th>Cleaned</th>\n",
              "      <th>Stemmed</th>\n",
              "      <th>Lemmatized</th>\n",
              "      <th>POS Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The movie was fantastic and I loved every part...</td>\n",
              "      <td>[The, movie, was, fantastic, and, I, loved, ev...</td>\n",
              "      <td>[movie, fantastic, loved, every, part, egypt]</td>\n",
              "      <td>[movi, fantast, love, everi, part, egypt]</td>\n",
              "      <td>[movie, fantastic, loved, every, part, egypt]</td>\n",
              "      <td>[(movie, NN), (fantastic, JJ), (loved, VBN), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I hated the film, it was the worst I have ever...</td>\n",
              "      <td>[I, hated, the, film, ,, it, was, the, worst, ...</td>\n",
              "      <td>[hated, film, worst, ever, seen]</td>\n",
              "      <td>[hate, film, worst, ever, seen]</td>\n",
              "      <td>[hated, film, worst, ever, seen]</td>\n",
              "      <td>[(hated, VBN), (film, NN), (worst, JJS), (ever...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The storyline was boring but the acting was br...</td>\n",
              "      <td>[The, storyline, was, boring, but, the, acting...</td>\n",
              "      <td>[storyline, boring, acting, brilliant]</td>\n",
              "      <td>[storylin, bore, act, brilliant]</td>\n",
              "      <td>[storyline, boring, acting, brilliant]</td>\n",
              "      <td>[(storyline, NN), (boring, VBG), (acting, VBG)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>An amazing movie with a great plot and incredi...</td>\n",
              "      <td>[An, amazing, movie, with, a, great, plot, and...</td>\n",
              "      <td>[amazing, movie, great, plot, incredible, perf...</td>\n",
              "      <td>[amaz, movi, great, plot, incred, perform]</td>\n",
              "      <td>[amazing, movie, great, plot, incredible, perf...</td>\n",
              "      <td>[(amazing, JJ), (movie, NN), (great, JJ), (plo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Egypt movie, I regret wasting my time on it</td>\n",
              "      <td>[Egypt, movie, ,, I, regret, wasting, my, time...</td>\n",
              "      <td>[egypt, movie, regret, wasting, time]</td>\n",
              "      <td>[egypt, movi, regret, wast, time]</td>\n",
              "      <td>[egypt, movie, regret, wasting, time]</td>\n",
              "      <td>[(egypt, JJ), (movie, NN), (regret, VB), (wast...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b472edc-68ad-410c-86b3-2e0a6162b8b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b472edc-68ad-410c-86b3-2e0a6162b8b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b472edc-68ad-410c-86b3-2e0a6162b8b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-00748a27-d7b2-4012-85ef-9417386f3878\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00748a27-d7b2-4012-85ef-9417386f3878')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-00748a27-d7b2-4012-85ef-9417386f3878 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Original Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n          \"I hated the film, it was the worst I have ever seen\",\n          \"The actors did a great job but the story lacked depth\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tokenized\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stemmed\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lemmatized\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POS Tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-RXL-C0o1Pd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kGDI-qHn9oGN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}